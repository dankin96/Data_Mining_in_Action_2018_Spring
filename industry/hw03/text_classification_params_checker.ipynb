{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.878361397336 {'countvectorizer__max_df': 0.31, 'countvectorizer__min_df': 1, 'countvectorizer__ngram_range': (1, 2), 'countvectorizer__stop_words': 'english', 'logisticregression__C': 20, 'logisticregression__class_weight': None, 'logisticregression__fit_intercept': False, 'logisticregression__solver': 'newton-cg', 'tfidftransformer__norm': 'l2', 'tfidftransformer__smooth_idf': 0, 'tfidftransformer__sublinear_tf': True}\n",
      "CPU times: user 8.87 s, sys: 1.33 s, total: 10.2 s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data = fetch_20newsgroups(\n",
    "    subset='all', \n",
    "    categories=[\n",
    "        'rec.autos',\n",
    "        'rec.motorcycles',\n",
    "        'rec.sport.baseball',\n",
    "        'rec.sport.hockey'\n",
    "    ], \n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    'countvectorizer__ngram_range' : [(1, 2)],\n",
    "    'countvectorizer__stop_words': ['english'],\n",
    "    'countvectorizer__min_df' : [1],\n",
    "    'countvectorizer__max_df' : [0.31],\n",
    "    'tfidftransformer__norm' : [\"l2\"],\n",
    "    'tfidftransformer__smooth_idf': [0],\n",
    "    'tfidftransformer__sublinear_tf': [True],\n",
    "    'logisticregression__C' : [20, 21],\n",
    "    'logisticregression__fit_intercept' : [False],\n",
    "    'logisticregression__solver' : ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "    'logisticregression__class_weight': [None],\n",
    "}\n",
    "\n",
    "pipeline = make_pipeline(CountVectorizer(),\n",
    "                     TfidfTransformer(),\n",
    "                     LogisticRegression())\n",
    "grid_search = GridSearchCV(pipeline, parameters, \n",
    "                         scoring='accuracy',\n",
    "                         n_jobs=-1, cv=3,\n",
    "                         verbose=1).fit(data.data, data.target)\n",
    "print(grid_search.best_score_, grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.878358477277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "import signal\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "\n",
    "SCRIPT_DIR = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "\n",
    "def signal_handler(signum, frame):\n",
    "    raise Exception(\"Timed out!\")\n",
    "\n",
    "\n",
    "class Checker(object):\n",
    "    def __init__(self):\n",
    "        self.data = fetch_20newsgroups(\n",
    "            subset='all', \n",
    "            categories=[\n",
    "                'rec.autos',\n",
    "                'rec.motorcycles',\n",
    "                'rec.sport.baseball',\n",
    "                'rec.sport.hockey'\n",
    "            ], \n",
    "            remove=('headers', 'footers', 'quotes')\n",
    "        )\n",
    "\n",
    "    def check(self, params_path):\n",
    "        try:\n",
    "            with open(params_path, 'r') as f:\n",
    "                params = json.load(f)\n",
    "\n",
    "            signal.signal(signal.SIGALRM, signal_handler)\n",
    "            signal.alarm(60)\n",
    "            pipeline = make_pipeline(\n",
    "                CountVectorizer(**params['count_vectorizer_params']), \n",
    "                TfidfTransformer(**params['tfidf_transformer_params']), \n",
    "                LogisticRegression(**params['logistic_regression_params'])\n",
    "            )\n",
    "            score = np.mean(cross_val_score(\n",
    "                pipeline, \n",
    "                self.data.data, \n",
    "                self.data.target,\n",
    "                scoring='accuracy', \n",
    "                cv=3\n",
    "            ))\n",
    "        except:\n",
    "            traceback.print_exception(*sys.exc_info())\n",
    "            score = None\n",
    "        \n",
    "        return score\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(Checker().check(SCRIPT_DIR + '/text_classification_params_danilov.json'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
